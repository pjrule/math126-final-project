{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16f708-d9f2-4f8f-a8b7-1da314bf8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm scipy scikit-learn pandas h5py matplotlib ksvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfeffed-4b24-4b03-add5-7b3dc6693e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import h5py\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import fft\n",
    "from scipy.signal.windows import hamming\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d8d28-8105-44a3-8333-500e22ea64b3",
   "metadata": {},
   "source": [
    "### Global (hyper)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0697b-908d-429a-9e9d-9c9be1c996dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets and cache\n",
    "dataset_path = '/Users/pjrule/Downloads/musicnet.h5'\n",
    "dataset_meta_path = '/Users/pjrule/Downloads/musicnet_metadata.csv'\n",
    "fingerprints_cache_path = './fingerprints.npz'  # WARNING: not currently refreshed across parameter changes\n",
    "grad_model_cache_path = './grad_model_full.joblib'   # WARNING: not currently refreshed across parameter changes\n",
    "\n",
    "# FFT parameters\n",
    "fs = 44100  # 44.1 kHz sample rate\n",
    "window_size = 1024\n",
    "sample_interval_sec = 0.05  # in seconds\n",
    "chunk_size = 30  # in seconds\n",
    "\n",
    "# model training parameters\n",
    "random_state = 0\n",
    "test_size = 0.2  # train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21fd75b-e665-4e06-90fa-67bcce4e7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_interval = int(sample_interval_sec * fs)\n",
    "intervals_per_chunk = int(chunk_size / sample_interval_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c409b07-9b87-4be2-9408-88d1b826017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File(dataset_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3620bb-cb74-4810-83e6-4f3582812311",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(dataset_meta_path).set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c75075-e637-42b8-8a51-98fb6b0330e6",
   "metadata": {},
   "source": [
    "## Fingerprinting\n",
    "For each recording, we generate an _audio fingerprint matrix_ by capturing the (truncated) audio spectrogram at a fixed interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb5a97-e2fd-4b28-ab3b-df74e3456732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_fingerprint(audio: np.ndarray):\n",
    "  \"\"\"Takes a fingerprint of an audio signal by sampling the spectogram at a fixed interval.\"\"\"\n",
    "  n_samples = audio.shape[0] // sample_interval\n",
    "  fingerprint = np.empty((n_samples, window_size // 8))\n",
    "  window = hamming(window_size)\n",
    "  for sample_idx in range(n_samples):\n",
    "    sample = audio[sample_idx * window_size:(sample_idx + 1) * window_size]\n",
    "    sample_mag = np.abs(sample)\n",
    "    if sample_mag.max() > 0:\n",
    "      normalized_sample = sample / sample_mag.max()\n",
    "    else:\n",
    "      normalized_sample = sample\n",
    "    windowed_sample = window * normalized_sample\n",
    "    fingerprint[sample_idx] = np.abs(fft(windowed_sample))[:window_size//8]\n",
    "  return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6071af3-978d-4fe8-b9c2-287f03f1566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_fingerprint(labels: np.ndarray):\n",
    "  \"\"\"TODO – use interval tree\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436a930-1643-4456-8f95-e41d05b81a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  fingerprints_by_id = np.load(fingerprints_cache_path)\n",
    "except FileNotFoundError:\n",
    "  fingerprints_by_id = {}\n",
    "  for key in tqdm(dataset):\n",
    "    fingerprints_by_id[key.split('id_')[1]] = audio_fingerprint(dataset[key]['data'][:])\n",
    "  np.savez_compressed(fingerprints_cache_path, **fingerprints_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acf951-bdce-48a4-92d4-8633499204fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(dataset['id_2572/data'][:], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6e3d1-4f9a-4830-a3c8-3914cba79b4d",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "The recordings in the MusicNet dataset are of variable length. For our purposes, we can devise a more interesting dataset (with more unique examples) by breaking the MusicNet recordings into fixed-length chunks. Because no musical passage should repeat _exactly_, overfitting shouldn't be an enormous concern (at least for first-order exploratory work); if we see evidence of overfitting, we can split into training/test sets at the recording level instead of the chunk level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e9cdb-4561-40e9-80bc-a33172f5c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprint_chunks_by_column(col: str):\n",
    "  ids_by_col = {\n",
    "    label: set(meta_df.iloc[idx].name for idx in indices)\n",
    "    for label, indices in meta_df.groupby(col).indices.items()\n",
    "  }\n",
    "  label_to_id = {label: idx for idx, label in enumerate(ids_by_col)}\n",
    "  \n",
    "  chunks = []\n",
    "  chunk_label_ids = []\n",
    "  for label, ids in ids_by_col.items():\n",
    "    for recording_id in ids:\n",
    "      recording_fingerprints = fingerprints_by_id[str(recording_id)]\n",
    "      for pos in range(0, len(recording_fingerprints), intervals_per_chunk):\n",
    "        chunk = recording_fingerprints[pos:pos + intervals_per_chunk]\n",
    "        if chunk.shape[0] == intervals_per_chunk:  # exclude partial chunks (at end)\n",
    "          chunks.append(chunk)\n",
    "          chunk_label_ids.append(label_to_id[label])\n",
    "  return np.array(chunks), np.array(chunk_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba490d-1dd0-4f98-89f7-41c64b7332df",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, chunk_labels = fingerprint_chunks_by_column('ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f34c1c-53c3-4b84-b977-b629a320f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e2f30-f34a-44d3-b64e-45e0595df7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(chunks[1400]).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926dd4f-a3c6-4a39-a96b-4f9c8c2477fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks_train, chunks_test, chunk_labels_train, chunk_labels_test = train_test_split(\n",
    "  chunks,\n",
    "  chunk_labels,\n",
    "  test_size=test_size,\n",
    "  random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4193fec-c9cb-4e2a-9b42-162835260374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_samples(chunks, chunk_labels):\n",
    "  samples = chunks.reshape(chunks.shape[0] * chunks.shape[1], -1)\n",
    "  sample_labels = chunk_labels.repeat(chunks.shape[1])\n",
    "  return samples, sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b56ca0-3e2f-4f92-b76e-2ba5079b2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train, sample_labels_train = chunks_to_samples(chunks_train, chunk_labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06862234-b3f2-46ac-99dc-a8a7612550a5",
   "metadata": {},
   "source": [
    "## Baseline models\n",
    "* Logistic regression\n",
    "* Gradient boosting\n",
    "* KSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00e4bb-b3be-449f-ad95-cf68a12f9d6f",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae4a35-f728-4399-a081-7b26cb65ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  grad_model = \n",
    "except FileNotFoundError:\n",
    "  grad_model = GradientBoostingClassifier(random_state=random_state, verbose=True)\n",
    "  grad_model.fit(samples_train, sample_labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ca82e-a2eb-4228-8e9d-66d8d4a9472d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment matrix\n",
    "Labels: instrumentation/ensemble, key, composer (Eric says: most interesting!), composer region, composer era\n",
    "\n",
    "Classifier: dictionary learning via randomized LU (two variants—say, tuning $k$ and $\\ell$; RRLU?), linear classifier, MLP\n",
    "\n",
    "Representation: audio fingerprints (FFT), MIDI\n",
    "\n",
    "Benchmark: accuracy, speed (wall clock), (if feasible) approximate # of FLOPs\n",
    "\n",
    "Breakdown:\n",
    "* Parker will draft a training/test pipeline and fine-tune our current randomized LU implementation\n",
    "* Eric will implement the dictionary classifier (inputs: randomized LU parameters & FFT/MIDI dictionary vectors; uses: randomized LU implementation; outputs: compressed dictionaries)\n",
    "* Zoe will port linear model/MLP model from MusicNet tutorials\n",
    "\n",
    "TODO: can we fit in rank-deficient least squares?\n",
    "\n",
    "Other random things:\n",
    "* Parker will subset the data to make it easier to play with locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781c118-996c-4dae-aa46-82098fec51cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
